{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap Data [SQL]\n",
    "> __UDACITY Data Analyst Nanodegree: Project 3__\n",
    "<br>\n",
    "__Patrick Ferry__\n",
    "<br>\n",
    "__December 2017__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Table of Contents\n",
    "\n",
    "[1. Project Objective](#po)<br>\n",
    "[2. Data Preparation](#dp)<br>\n",
    "[3. Process Dataset](#pd)<br>\n",
    "[4. Audit and Clean Data](#acd)<br>\n",
    "[5. Convert Dataset from XML to CSV](#cdxc)<br>\n",
    "[6. Import CSV Files into SQL Database](#icfisd)<br>\n",
    "[7. Explore Database](#ed)<br>\n",
    "[8. Conclusion / Other Ideas](#oi)<br>\n",
    "[9. Files](#f)<br>\n",
    "[10. References](#r)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='po'></a>\n",
    "## 1. Project Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose any area of the world from [OpenStreetMap](https://www.openstreetmap.org) and use data munging techniques, such as assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity, to clean the OpenStreetMap data for that part of the world. Use SQL as the data schema to complete the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills Acquired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal upon completion of the case study is to improve my understanding and application of the following:\n",
    "\n",
    "-  Assessing quality of data for validity, accuracy, completeness, consistency and uniformity\n",
    "-  Parsing and gathering data from popular file formats such as .csv, .json, .xml, and .html\n",
    "-  Processing data from multiple files, or very large files, that can be cleaned programmatically\n",
    "-  Storing, querying, and aggregating data using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the map area of the city of Salem, MA because that is where I grew up.  I am familiar with the area and curious to discover any anomolies or surprises since moving to NYC in 1996.\n",
    "\n",
    "The first issue I encountered when attempting to download a map of [Salem, MA](https://www.openstreetmap.org/relation/2373035) from OpenStreetMap, was that it was last edited approximately 5 years ago.  I anticipate errors, omissions, or at the minimum outdated data.  (There doesn't seem to be a lot of interest in this area of the world, even though most people do know Salem from the historical Salem Witch Trials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"witch.jpg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second issue occurred when the export failed, perhaps due to a moved, deleted or renamed file.\n",
    "\n",
    "Ultimately I downloaded a custom extract of the greater Salem area from [Mapzen's Metro Extracts](https://mapzen.com/data/metro-extracts/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=orange>Map of Salem, MA:</font>\n",
    "![alt text](Salem_MA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenStreetMap's data is structured in well-formed XML documents (.osm files) that consist of the following elements:\n",
    "\n",
    "-  __Nodes__: \"Nodes\" are individual dots used to mark specific locations (such as a postal box). Two or more nodes are used to draw line segments or \"ways\".\n",
    "-  __Ways__: A \"way\" is a line of nodes, displayed as connected line segments. \"Ways\" are used to create roads, paths, rivers, etc.\n",
    "-  __Relations__: When \"ways\" or areas are linked in some way but do not represent the same physical thing, a \"relation\" is used to describe the larger entity they are part of. \"Relations\" are used to create map features, such as cycling routes, turn restrictions, and areas that are not contiguous. The multiple segments of a long way, such as an interstate or a state highway are grouped into a \"relation\" for that highway. Another example is a national park with several locations that are separated from each other. Those are also grouped into a \"relation\".\n",
    "\n",
    "All these elements can carry tags describing the name, type of road, and other attributes.\n",
    "\n",
    "Understanding this high level overview of the data I'd be wrangling informed me for future downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dp'></a>\n",
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries, packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET   # Use cElementTree or lxml if too slow\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "import random\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import os\n",
    "from hurry.filesize import size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSM file preparation generating a sample data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial phases of the project I worked with a smaller, sample OSM file size to check for the most common problems to clean. The final step in the project includes queries form the larger, original OSM file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code from Udacity Project Details section\n",
    "\n",
    "OSM_FILE = \"salem_ma.osm\"\n",
    "SAMPLE_FILE = \"salem_sample.osm\"\n",
    "\n",
    "k = 3 # take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "            \n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "    \n",
    "    # Write every k-th top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Udacity code above, the smaller sample size OSM file (“salem_sample.osm”, 16.4 MB) allows for faster processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pd'></a>\n",
    "## 3. Process Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data exploration phase entailed familiarizing myself with the data and exploring it in order to gain a good understanding of the OpenStreetMap project in order to complete the case study.\n",
    "\n",
    "Running the shell command “ls –l” on the original OSM file revealed the size of the file (“salem_ma.osm”, 162.4 MB), which is relevant later on when choosing an approach to parsing. Running the shell command “less” returned the meta format of the dataset, allowing for a forensic overview.\n",
    "\n",
    "Familiarity with the dataset improved after reading through the [wiki OSM XML documentation](http://wiki.openstreetmap.org/wiki/OSM_XML) that provided a relational understanding of nodes, ways and relations and how I might work with the dataset. The data contains instances of three different data primitives (nodes, ways, relations).\n",
    "\n",
    "Before processing the data and adding it into a database, I was interested in sorting out patterns and uniqueness. One way is to find top-level tags with key-value pairs in the OSM dataset, i.e. all distinct different types of tags (OSM, bounds, node, tag). Another is to count four other tag categories and check the k-value for each to see if there are any potential problems. A third is to find out how many unique users have contributed to the map.\n",
    "\n",
    "Because the original file is so large, it didn't make sense to process the data by reading it into memory.  Rather than using tree-based XML parsing, which reads the entire document into memory and works with it as nodes on a tree, I chose an approach to parsing that uses a SAX parser, or iterative parsing.\n",
    "\n",
    "Using the [iterparse()](https://docs.python.org/2/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse) function, I looped through the dataset creating dictionaries for the above three mentioned instances:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag type and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'member': 401,\n",
       "             'nd': 275384,\n",
       "             'node': 240142,\n",
       "             'osm': 1,\n",
       "             'relation': 49,\n",
       "             'tag': 88171,\n",
       "             'way': 30319})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code from Udacity Case Study [SQL] \"Iterative Parsing\"\n",
    "\n",
    "def count_tags(filename):\n",
    "    mytags =  defaultdict(int)\n",
    "    for event, child in ET.iterparse(filename):\n",
    "        if child.tag in mytags:\n",
    "            mytags[child.tag] += 1\n",
    "        else:\n",
    "            mytags[child.tag] = 1\n",
    "    \n",
    "    return mytags\n",
    "    \n",
    "count_tags(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pd'></a>\n",
    "### Other tags, k-values and patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 78122, 'lower_colon': 3884, 'other': 6165, 'problemchars': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code from Udacity Case Study [SQL] \"Tag Types\"\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if lower.search(element.attrib['k']):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif lower_colon.search(element.attrib['k']):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "    \n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    \n",
    "    return keys\n",
    "\n",
    "process_map(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pd'></a>\n",
    "### Unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 217\n"
     ]
    }
   ],
   "source": [
    "# Code from Udacity Case Study [SQL] \"Exploring Users\"\n",
    "\n",
    "def get_user(element):\n",
    "    uid = element.attrib['uid']\n",
    "    return uid\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = [] #set()\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        if elem.attrib.get('uid'):\n",
    "            users.append(get_user(elem))\n",
    "    users = set(users)\n",
    "        \n",
    "    return users\n",
    "\n",
    "users = process_map(SAMPLE_FILE)\n",
    "print \"Number of unique users:\",\n",
    "print len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='acd'></a>\n",
    "## 4. Audit and Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a good feel for how the OSM XML data was organized, and what certain tags described, was important when parsing the data.  I found that within our three data primitives (nodes, ways and relations) both nodes and ways can be tagged.\n",
    "\n",
    "In this phase of the project there are two steps to audit and clean the data.  \n",
    "\n",
    "First, audit the OSMFILE, using iterparse from the Element Tree module to loop through the entire dataset.  I audited 3 specific attributes (street names, postal codes, city values) and checked the values associated with each attribute to determine what needed to be cleaned.\n",
    "\n",
    "Second, write an “update()” function to fix any incorrect data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example of ways, I used iterparse() to get the number of street types and check for problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Street types and count:\n",
      "\n",
      "\n",
      "Ave: 3\n",
      "Avenue: 60\n",
      "Avenur: 1\n",
      "Circle: 6\n",
      "Court: 1\n",
      "Drive: 12\n",
      "Lane: 19\n",
      "Lynnway: 1\n",
      "North: 1\n",
      "Place: 7\n",
      "Road: 108\n",
      "SalemStreet: 1\n",
      "St: 2\n",
      "St.: 1\n",
      "street: 1\n",
      "Street: 119\n",
      "Terrace: 7\n",
      "Way: 14\n"
     ]
    }
   ],
   "source": [
    "# Code from Udacity Data Quality \"Example Using Our Blueprint\"\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)  # regular expression modual to parse out the street types\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        street_types[street_type] +=1\n",
    "        \n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print \"%s: %d\" % (k, v)\n",
    "        \n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit():\n",
    "    for event, elem in ET.iterparse(SAMPLE_FILE):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "    print_sorted_dict(street_types)\n",
    "\n",
    "print \"Street types and count:\"\n",
    "print \"\\n\"\n",
    "audit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I called the “audit_street_type” function, which uses regular expression to match the last word in a street name with the street type (Ave, Blvd, St).  If the street type I found for this particular tag was incorrect, it was added to the “street_types” dictionary for unusual streets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem street names:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'Ave': {'Highland Ave', 'Loring Ave'},\n",
       "             'Avenur': {'Harrison Avenur'},\n",
       "             'SalemStreet': {'SalemStreet'},\n",
       "             'St': {'Lafayette St', 'W Dane St'},\n",
       "             'St.': {'Essex St.'},\n",
       "             'street': {'green street'}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code from Udacity Case Study \"Improving Street Names\"\n",
    "\n",
    "expected = [\"Avenue\", \"Circle\", \"Court\", \"Drive\", \"Lane\", \"Lynnway\", \"North\", \"Place\", \"Road\", \"Square\", \"Street\", \n",
    "            \"Terrace\", \"Way\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)   # regular expression modual to parse out the street types\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    \n",
    "    return street_types\n",
    "\n",
    "print \"Problem street names:\"\n",
    "\n",
    "audit(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems encountered that necessitated cleaning included street names (abbreviation, misspelling, capitalization, spacing), postal codes (uniformity, non-Salem zips) and city values (surrounding towns).\n",
    "\n",
    "The variable 'mapping' was then changed to reflect updates needed to fix the unexpected street types to correct ones in the expected list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update street names:\n",
      "\n",
      "\n",
      "SalemStreet --> Salem Street\n",
      "Essex St. --> Essex Street\n",
      "Harrison Avenur --> Harrison Avenue\n",
      "W Dane St --> West Dane Street\n",
      "Lafayette St --> Lafayette Street\n",
      "green street --> Green Street\n",
      "Highland Ave --> Highland Avenue\n",
      "Loring Ave --> Loring Avenue\n"
     ]
    }
   ],
   "source": [
    "# Code from Udacity Case Study \"Improving Street Names\"\n",
    "\n",
    "mapping = { \"Ave\": \"Avenue\",\n",
    "           \"Avenur\": \"Avenue\", \n",
    "           \"SalemStreet\": \"Salem Street\",\n",
    "           \"St\": \"Street\",\n",
    "           \"St.\": \"Street\",\n",
    "           \"street\": \"Street\",\n",
    "           \"W\": \"West\",\n",
    "            }\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    name = name.split(\" \")\n",
    "    for word in range(len(name)):\n",
    "        if name[word] in mapping.keys():\n",
    "            name[word] = mapping[name[word]]\n",
    "    name = \" \".join(name)\n",
    "    \n",
    "    return name\n",
    "\n",
    "print \"Update street names:\"\n",
    "print \"\\n\"\n",
    "\n",
    "st_types = audit(SAMPLE_FILE)\n",
    "# pprint.pprint(dict(st_types))\n",
    "\n",
    "for st_types, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping).title()\n",
    "        print name, \"-->\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar auditing, cleaning and updating code was applied to postal codes and city values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postal codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem postal codes:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['01901-1511', '01907-2555']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code adapted from Udacity \"Improving Street Names\"\n",
    "\n",
    "zip_codes = []\n",
    "\n",
    "def is_zip_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zip_codes(zip_code):\n",
    "    if zip_code[:3] != \"019\" or len(zip_code) != 5:\n",
    "        zip_codes.append(zip_code)\n",
    "        \n",
    "def audit_zip(osmfile):\n",
    "    for event, elem in ET.iterparse(SAMPLE_FILE, events=(\"start\",)):\n",
    "        if elem.tag == \"node\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zip_code(tag):\n",
    "                    audit_zip_codes(tag.attrib['v'])\n",
    "    return zip_codes\n",
    "\n",
    "print \"Problem postal codes:\"\n",
    "\n",
    "audit_zip(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update postal codes:\n",
      "\n",
      "\n",
      "01901\n",
      "01907\n"
     ]
    }
   ],
   "source": [
    "# Code from Myles, Udacity Forum Member, in \"Cleaning Postcode\" Oct 2016\n",
    "\"\"\"\n",
    "Update pattern using regex.  Piece together regular expression pattern:\n",
    "    ^ denotes match at the start\n",
    "    \\D represents non-digit characters\n",
    "    * represents 'from zero to an infinite number'\n",
    "    \\d represents digit characters\n",
    "    () represents the group that you want to capture\n",
    "    . represents 'any character'\n",
    "\"\"\"\n",
    "\n",
    "test = ['01901-1511', '01907-2555']\n",
    "\n",
    "def update_postcode(postcode):\n",
    "    # new regular expression pattern\n",
    "    search = re.match(r'^\\D*(\\d{5}).*', postcode)\n",
    "    # select the group that is captured\n",
    "    clean_postcode = search.group(1)\n",
    "    return clean_postcode\n",
    "\n",
    "print \"Update postal codes:\"\n",
    "print \"\\n\"\n",
    "\n",
    "for item in test:\n",
    "    cleaned = update_postcode(item)\n",
    "    print cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem city names:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Beverly': 6,\n",
       "             'Danvers': 15,\n",
       "             'Lynn': 10,\n",
       "             'Marblehead': 1,\n",
       "             'Peabody': 8,\n",
       "             'Swampscott': 22})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code from Udacity (https://discussions.udacity.com/t/need-some-help-with-update-city-name-function/262878)\n",
    "\n",
    "expected = [\"Salem\"]\n",
    "\n",
    "def audit_city(invalid_city_names, city_name):\n",
    "    if city_name not in expected:\n",
    "        invalid_city_names[city_name] +=1\n",
    "    return invalid_city_names\n",
    "\n",
    "def is_city_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:city\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    invalid_city_names = defaultdict(int)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_city_name(tag):\n",
    "                    audit_city(invalid_city_names, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return invalid_city_names\n",
    "\n",
    "print \"Problem city names:\"\n",
    "\n",
    "audit(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem cities were not a problem nor required updating, as each accurately represents a surrounding city in the Greater Salem area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cdxc'></a>\n",
    "## 5. Convert Dataset from XML to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial auditing and updating was complete, the next involved preparing the data from the original OSM file to be inserted into an SQL database. This required parsing the elements in the OSM file and transforming them from document format to tabular format, thus making it possible to write to .csv files.  The .csv files are then imported into an SQL database as tables.\n",
    "\n",
    "When parsing and cleaning the OSM file, the ‘process_map’ function passes each parent element of the OSM file to the ‘shape_element’ function.  The ‘shape_element’ function parses the information from each parent element, and its children, into dictionaries.  The dictionaries are iteratively processed by ‘process_map’ function and .csv files are generated.\n",
    "\n",
    "*In addition to defining a schema for the .csv files and eventual tables, the code to load the data, perform iterative parsing and write the output to .csv files was provided by Udacity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/pratyush19/Udacity-Data-Analyst-Nanodegree/blob/master/P3-OpenStreetMap-Wrangling-with-SQL/data.py\n",
    "\n",
    "OSM_PATH = \"salem_ma.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  \n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in NODE_FIELDS:\n",
    "                node_attribs[attrib] = element.attrib[attrib]\n",
    "        \n",
    "        for child in element:\n",
    "            node_tag = {}\n",
    "            if LOWER_COLON.match(child.attrib['k']):\n",
    "                node_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                node_tag['key'] = child.attrib['k'].split(':',1)[1]\n",
    "                node_tag['id'] = element.attrib['id']\n",
    "                if child.attrib[\"k\"] == 'addr:street':\n",
    "                    node_tag[\"value\"] = update_name(child.attrib[\"v\"], mapping)\n",
    "                elif child.attrib[\"k\"] == 'addr:postcode':\n",
    "                    node_tag[\"value\"] = update_postcode(child.attrib[\"v\"])\n",
    "                else:\n",
    "                    node_tag['value'] = child.attrib['v']\n",
    "                tags.append(node_tag)\n",
    "            elif PROBLEMCHARS.match(child.attrib['k']):\n",
    "                continue\n",
    "            else:\n",
    "                node_tag['type'] = 'regular'\n",
    "                node_tag['key'] = child.attrib['k']\n",
    "                node_tag['id'] = element.attrib['id']\n",
    "                node_tag['value'] = child.attrib['v']\n",
    "                tags.append(node_tag)\n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    elif element.tag == 'way':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in WAY_FIELDS:\n",
    "                way_attribs[attrib] = element.attrib[attrib]\n",
    "        \n",
    "        position = 0\n",
    "        for child in element:\n",
    "            way_tag = {}\n",
    "            way_node = {}\n",
    "            \n",
    "            if child.tag == 'tag':\n",
    "                if LOWER_COLON.match(child.attrib['k']):\n",
    "                    way_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                    way_tag['key'] = child.attrib['k'].split(':',1)[1]\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['value'] = child.attrib['v']\n",
    "                    tags.append(way_tag)\n",
    "                elif PROBLEMCHARS.match(child.attrib['k']):\n",
    "                    continue\n",
    "                else:\n",
    "                    way_tag['type'] = 'regular'\n",
    "                    way_tag['key'] = child.attrib['k']\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['value'] = child.attrib['v']\n",
    "                    tags.append(way_tag)\n",
    "                    \n",
    "            elif child.tag == 'nd':\n",
    "                way_node['id'] = element.attrib['id']\n",
    "                way_node['node_id'] = child.attrib['ref']\n",
    "                way_node['position'] = position\n",
    "                position += 1\n",
    "                way_nodes.append(way_node)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CONVERTING DATA FROM XML TO CSV:\n",
      "--------------------------------\n",
      "File Conversion Completed\n"
     ]
    }
   ],
   "source": [
    "# Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "# sample of the map when validating.\n",
    "\n",
    "process_map(OSM_PATH, validate=True)\n",
    "print \"\\n\"\n",
    "print \"CONVERTING DATA FROM XML TO CSV:\"\n",
    "print \"--------------------------------\"\n",
    "print \"File Conversion Completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CSV files produced__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes.csv\n",
      "nodes_tags.csv\n",
      "ways.csv\n",
      "ways_nodes.csv\n",
      "ways_tags.csv\n"
     ]
    }
   ],
   "source": [
    "filenames = find_csv_filenames(\"/Users/jennieferry/Desktop/pmf/udacity/Udacity II/5_Data_Wrangling/P3_Wrangle OpenStreetMap Data/revision\")\n",
    "for name in filenames:\n",
    "    print name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='icfisd'></a>\n",
    "## 6. Import CSV Files into SQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps*:\n",
    "\n",
    "1. Import the modules that you need\n",
    "2. Connect to the database\n",
    "3. Create a cursor object\n",
    "4. Create the table\n",
    "5. Read in the data\n",
    "6. Insert the data\n",
    "7. Check that the data imported correctly\n",
    "8. Close the connection\n",
    "\n",
    "*https://discussions.udacity.com/t/creating-db-file-from-csv-files-with-non-ascii-unicode-characters/174958/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create nodes table\n",
    "\n",
    "sqlite_file = 'pmfdb.db'\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "conn.commit()\n",
    "cur.execute('''CREATE TABLE nodes(id INTEGER, lat TEXT, lon TEXT, user TEXT, uid INTEGER, version TEXT, changeset TEXT, timestamp TEXT)''')\n",
    "conn.commit()\n",
    "with open('nodes.csv','rb') as fin:\n",
    "        dr = csv.DictReader(fin) # comma is default delimiter\n",
    "        to_db = [(i['id'].decode(\"utf-8\"), i['lat'].decode(\"utf-8\"), i['lon'].decode(\"utf-8\"), i['user'].decode(\"utf-8\"), i[\"uid\"].decode(\"utf-8\"), i[\"version\"].decode(\"utf-8\"),i[\"changeset\"].decode(\"utf-8\"),i[\"timestamp\"].decode(\"utf-8\")) for i in dr]\n",
    "cur.executemany(\"INSERT INTO nodes (id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?,?,?);\", to_db)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create nodes_tags table\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "conn.commit()\n",
    "cur.execute('''CREATE TABLE nodes_tags(id INTEGER, key TEXT, value TEXT,type TEXT)''')\n",
    "conn.commit()\n",
    "with open('nodes_tags.csv','rb') as fin:\n",
    "        dr = csv.DictReader(fin) # comma is default delimiter\n",
    "        to_db = [(i['id'].decode(\"utf-8\"), i['key'].decode(\"utf-8\"), i['value'].decode(\"utf-8\"), i['type'].decode(\"utf-8\")) for i in dr]\n",
    "cur.executemany(\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create ways table\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute('''DROP TABLE IF EXISTS ways''')\n",
    "conn.commit()\n",
    "cur.execute('''CREATE TABLE ways(id INTEGER, user TEXT, uid TEXT, version TEXT, changeset TEXT, timestamp TEXT)''')\n",
    "conn.commit()\n",
    "with open('ways.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['user'].decode(\"utf-8\"), i['uid'].decode(\"utf-8\"), i['version'].decode(\"utf-8\"), i['changeset'].decode(\"utf-8\"), i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "cur.executemany(\"INSERT INTO ways (id, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?);\", to_db)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create ways_tags table\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "conn.commit()\n",
    "cur.execute('''CREATE TABLE ways_tags(id INTEGER, key TEXT, value TEXT, type TEXT)''')\n",
    "conn.commit()\n",
    "with open('ways_tags.csv','rb') as fin:\n",
    "        dr = csv.DictReader(fin) # comma is default delimiter\n",
    "        to_db = [(i['id'].decode(\"utf-8\"), i['key'].decode(\"utf-8\"), i['value'].decode(\"utf-8\"), i['type'].decode(\"utf-8\")) for i in dr]\n",
    "cur.executemany(\"INSERT INTO ways_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create ways_nodes table\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "conn.commit()\n",
    "cur.execute('''CREATE TABLE ways_nodes(id INTEGER, node_id INTEGER, position INTEGER)''')\n",
    "conn.commit()\n",
    "with open('ways_nodes.csv','rb') as fin:\n",
    "        dr = csv.DictReader(fin) # comma is default delimiter\n",
    "        to_db = [(i['id'].decode(\"utf-8\"), i['node_id'].decode(\"utf-8\"), i['position'].decode(\"utf-8\")) for i in dr]\n",
    "cur.executemany(\"INSERT INTO ways_nodes (id, node_id, position) VALUES (?,?,?);\", to_db)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ed'></a>\n",
    "## 7. Explore Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with cleaned .csv files and an understanding of the data inherent in those files, I set about exploring the dataset using SQL.  Queries on the data file, some top 10 instances, and other practical information about the map area helped me get a different perspective of my hometown; certain aspects I had never realized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlite_file = 'pmfdb.db'\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Number of unique users__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(423,)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT COUNT(DISTINCT(a.uid))\n",
    "FROM (SELECT DISTINCT(uid)\n",
    "FROM ways\n",
    "UNION ALL\n",
    "SELECT DISTINCT(uid)\n",
    "FROM nodes) as a;'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Number of nodes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720425,)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT COUNT(*) FROM nodes'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Number of ways__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90956,)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT COUNT(*) FROM ways'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Users (by number of entries)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'jremillard-massgis', 550515)\n",
      "(u'morganwahl', 95983)\n",
      "(u'crschmidt', 56495)\n",
      "(u'MassGIS Import', 38475)\n",
      "(u'Utible', 16054)\n",
      "(u'wambag', 10879)\n",
      "(u'Ahlzen', 10095)\n",
      "(u'Alan Bragg', 6494)\n",
      "(u'OceanVortex', 4449)\n",
      "(u'dloutzen', 2809)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT nodes_ways.\"user\" AS \"User\", COUNT(*) AS \"Users\"\n",
    "FROM (SELECT \"user\" FROM nodes\n",
    "      UNION ALL\n",
    "      SELECT \"user\" FROM ways) AS nodes_ways\n",
    "GROUP BY nodes_ways.\"user\"\n",
    "ORDER BY \"Users\" DESC\n",
    "LIMIT 10;'''\n",
    "\n",
    "results = c.execute(query)\n",
    "\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Denominations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'baptist', 9)\n",
      "(u'methodist', 7)\n",
      "(u'pentecostal', 3)\n",
      "(u'episcopal', 2)\n",
      "(u'lutheran', 2)\n",
      "(u'catholic', 1)\n",
      "(u'mormon', 1)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT value, count(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key='denomination'\n",
    "GROUP BY value\n",
    "ORDER BY num\n",
    "DESC;'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cuisines__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'pizza', 5)\n",
      "(u'seafood', 2)\n",
      "(u'Vegetarian,Vegan', 1)\n",
      "(u'asian', 1)\n",
      "(u'breakfast,_diner', 1)\n",
      "(u'brunch', 1)\n",
      "(u'chinese', 1)\n",
      "(u'indian', 1)\n",
      "(u'mexican', 1)\n",
      "(u'seafood;american', 1)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT nodes_tags.value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "JOIN (SELECT DISTINCT(id)\n",
    "FROM nodes_tags\n",
    "WHERE value='restaurant') b\n",
    "ON nodes_tags.id=b.id\n",
    "WHERE nodes_tags.key='cuisine'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num\n",
    "DESC limit 10;'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__City values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Swampscott', 70)\n",
      "(u'Salem', 52)\n",
      "(u'Peabody', 36)\n",
      "(u'Lynn', 34)\n",
      "(u'Danvers', 32)\n",
      "(u'Beverly', 14)\n",
      "(u'Marblehead', 10)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT tags.value, COUNT(*) as count\n",
    "FROM (SELECT * FROM nodes_tags\n",
    "UNION ALL\n",
    "SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key='city'\n",
    "GROUP BY tags.value\n",
    "ORDER BY count\n",
    "DESC limit 12;'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Amenities__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'school', 124)\n",
      "(u'library', 53)\n",
      "(u'place_of_worship', 46)\n",
      "(u'bench', 28)\n",
      "(u'restaurant', 26)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key = 'amenity'\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 5;'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Popular Streets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Puritan Road', 125)\n",
      "(u'Humphrey Street', 113)\n",
      "(u'Stetson Avenue', 65)\n",
      "(u'Salem Street', 63)\n",
      "(u'Yankee Division Highway', 55)\n",
      "(u'Washington Street', 44)\n",
      "(u'Essex Street', 40)\n",
      "(u'Nason Road', 37)\n",
      "(u'Bradlee Avenue', 32)\n",
      "(u'Andover Street', 29)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT street_names.value AS \"Street\", COUNT(street_names.value) AS \"Times Refered\"\n",
    "FROM\n",
    "(SELECT nodes_tags.value\n",
    "FROM nodes_tags\n",
    "WHERE type = 'addr' AND key = 'street'\n",
    "UNION ALL\n",
    "SELECT ways_tags.value\n",
    "FROM ways_tags\n",
    "WHERE type = 'addr' AND key = 'street'\n",
    "OR\n",
    "id in\n",
    "(SELECT id\n",
    "FROM ways_tags\n",
    "WHERE key = 'highway')\n",
    "AND key = 'name') AS street_names\n",
    "GROUP BY street_names.value\n",
    "ORDER BY \"Times Refered\" DESC\n",
    "LIMIT 10;'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bus stops__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(853,)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT COUNT(*)\n",
    "FROM nodes_tags\n",
    "WHERE value = 'bus_stop';'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Starbucks__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT COUNT(DISTINCT(id)) \n",
    "FROM nodes_tags\n",
    "WHERE value=\"Starbucks\";'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Waterways__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Frost Fish Brook', 9)\n",
      "(u'Bass River', 7)\n",
      "(u'North River', 7)\n",
      "(u'Crane River', 6)\n",
      "(u'Breeds Pond Outlet Dam', 1)\n",
      "(u'Danvers River', 1)\n",
      "(u'Lynn Reservoir Dam', 1)\n",
      "(u'Marblehead Boatyard', 1)\n",
      "(u'Porter River', 1)\n",
      "(u'Saugus River', 1)\n",
      "(u'Sluice Pond Dam', 1)\n",
      "(u'South River', 1)\n",
      "(u'Walden Pond East End Dam', 1)\n",
      "(u'Waters River', 1)\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT ways_tags.value, COUNT(*) as num\n",
    "FROM ways_tags\n",
    "JOIN (SELECT DISTINCT(id)\n",
    "FROM ways_tags\n",
    "WHERE key='waterway') c\n",
    "ON ways_tags.id = c.id\n",
    "WHERE ways_tags.key = 'name'\n",
    "GROUP BY ways_tags.value\n",
    "ORDER BY num DESC;'''\n",
    "\n",
    "results = c.execute(query)\n",
    "for r in results: \n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='oi'></a>\n",
    "## 8. Conclusion / Other Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As happy as I was with the auditing and cleaning of data for Salem, MA I can't help but think Google is lightyears ahead with accurate and updated information.  I wonder if the Google Map developer site or API utilizes machine learning algorithms that audit, clean and update map information without the need of a human contributor.  The machine learning section is next.  Perhaps I'll circle back on the OSM data if there is a relevant project to test the assumption.\n",
    "\n",
    "I'd like to see more precise custom extracts for a particular city, similar to the Metro Extracts, as surrounding areas and related data are invariablly included.  The data was relatively clean, which surprised me since it was last edited 5 years ago.  Perhaps the most frequent contributers took their responsibility and had pride in accurately reflecting as much data as possible.\n",
    "\n",
    "Will we see an acquisition or merger between Google and OpenStreetMap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f'></a>\n",
    "## 9. Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md...............................: 1K   \n",
      "sample.py...............................: 1K   \n",
      "query.py................................: 2K   \n",
      "schema.py...............................: 2K   \n",
      "database.py.............................: 3K   \n",
      "audit.py................................: 5K   \n",
      "ways.csv................................: 5M   \n",
      "ways_tags.csv...........................: 5M   \n",
      "nodes_tags.csv..........................: 6M   \n",
      "data_revised.py.........................: 7K   \n",
      "ways_nodes.csv..........................: 18M  \n",
      "Report_revised.ipynb....................: 31K  \n",
      "salem_sample.osm........................: 52M  \n",
      "Wrangle_OSM_Final_revised.ipynb.........: 59K  \n",
      "nodes.csv...............................: 61M  \n",
      "pmfdb.db................................: 96M  \n",
      "salem_ma.osm............................: 154M \n",
      "Map.html................................: 244K \n",
      "References.html.........................: 244K \n",
      "Report_revised.html.....................: 289K \n",
      "parser.py...............................: 410B \n",
      "tags.py.................................: 964B \n"
     ]
    }
   ],
   "source": [
    "dirpath = \"/Users/jennieferry/Desktop/pmf/udacity/Udacity II/5_Data_Wrangling/P3_Wrangle OpenStreetMap Data/revision\"\n",
    "\n",
    "file_size_wanted = ['parser.py',\n",
    "'tags.py',\n",
    "'README.md',\n",
    "'sample.py',\n",
    "'schema.py',\n",
    "'query.py',\n",
    "'database.py',\n",
    "'audit.py',\n",
    "'data_revised.py',\n",
    "'Report_revised.ipynb',\n",
    "'Wrangle_OSM_Final_revised.ipynb',\n",
    "'Report_revised.pdf',\n",
    "'References.html',\n",
    "'Map.html',\n",
    "'Report_revised.html',\n",
    "'ways_tags.csv',\n",
    "'ways.csv',\n",
    "'nodes_tags.csv',\n",
    "'ways_nodes.csv',\n",
    "'salem_sample.osm',\n",
    "'nodes.csv',\n",
    "'pmfdb.db',\n",
    "'salem_ma.osm']\n",
    "\n",
    "files_list = []\n",
    "for path, dirs, files in os.walk(dirpath):\n",
    "    files_list.extend([(filename, size(os.path.getsize(os.path.join(path, filename)))) for filename in files])\n",
    "    files_list.sort(key=lambda size: int(size[1].translate(None, \"MKB\")))  # sort numerically\n",
    "    # files_list.sort(key = lambda letter: ''.join([i for i in letter[1] if not i.isdigit()]))  # sort quantifier\n",
    "    \n",
    "for filename, size in files_list:\n",
    "    if filename in file_size_wanted:\n",
    "        print '{:.<40s}: {:5s}'.format(filename,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='r'></a>\n",
    "## 10. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Udacity - [https://www.udacity.com/](https://www.udacity.com/)<br>\n",
    "Udacity Discussion Forum - [https://discussions.udacity.com/c/nd002-data-wrangling](https://discussions.udacity.com/c/nd002-data-wrangling)<br>\n",
    "OpenStreetMap Wiki - [https://wiki.openstreetmap.org/wiki/Main_Page](https://wiki.openstreetmap.org/wiki/Main_Page)<br>\n",
    "Github - [https://github.com/](https://github.com/)\n",
    " - [https://github.com/davidventuri](https://github.com/davidventuri)\n",
    " - [https://github.com/libiseller](https://github.com/libiseller)\n",
    " - [https://github.com/yudataguy](https://github.com/yudataguy)\n",
    " - [https://github.com/CassLamendola](https://github.com/CassLamendola)\n",
    " - [https://github.com/pratyush19](https://github.com/pratyush19)\n",
    " - [https://github.com/jasminej90](https://github.com/jasminej90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
